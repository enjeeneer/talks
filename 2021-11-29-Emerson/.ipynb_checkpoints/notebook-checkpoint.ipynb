{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Low Emission Building Control with Probabilistic Model-Based Reinforcement Learning\n",
    "\n",
    "## [Scott Jeen](https://enjeeneer.io/)\n",
    "\n",
    "### Emerson, Dayton OH\n",
    "### 29th November 2021\n",
    "\n",
    "<img class=\"\" src=\"images/logos.png\" align='left' style=\"width:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline \n",
    "1. Motivation\n",
    "    * Climate\n",
    "    * Commerical\n",
    "2. Existing Solutions\n",
    "3. Probabilistic Model-Based Reinforcment Learning\n",
    "4. Experiments\n",
    "5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here's a diagram my supervisor Jon Cullen produced a few years ago mapping global energy sources, on the left handside of the diagram, through conversion devices, to final services on the right handside of the diagram [1]. Though it looks complicated, I'd like to simplify things by focussing your attention on the middle section of the diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 Climate\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/global_sankey_1a.png?raw=true\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we see that global energy use can be stratified into one of three categories: energy used to create motion in \n",
    "cyan; energy used to regulate temperature in red; and energy used for everthing else in black. We can observe that the primary source of energy consumption globally is in regulating temperature - approximately 50\\% of total energy demand. Finding ways of regulating temperature in society more efficiently would prove a useful way of reducing energy use and this carbon emissions.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 Climate\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/global_sankey_3a.png?raw=true\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Another part of the diagram I'd like to draw your attention to is the blue sliver labelled renewables in the bottom left. Though renewables contributed to a small percentage of global energy supply when this diagram was produced (using data from 2005), the International Energy Agency expect renewables to contribute to 75\\% of global energy supply by 2050. Such a high penetration of renewables on the grid will cause problems, problems related primarily to the stochastic nature of renewable energy generation - often called its *intermittency*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 Climate\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/global_sankey_2a.png?raw=true\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To explain intermittency, consider the hypothetical scenario of powering a home using only a solar array located on its roof. Here, we plot demand for energy in blue and the supply of energy from the solar array in orange, for 24 hours. On a perfectly clear summers day, the energy supplied by our solar array will increase in the morning as the sun rises, peak around midday when irradiance is highest, and fall in the evening as the sun sets. In contrast, the occupant's demand for energy won't be so symmetric. They'll likely demand produce a peak in energy demand in the morning when they wake up, boil the kettle, turn the heating on and take a shower. Then, they'll head out to work and demand will fall and plateau. In the evening they'll return causing another spike in energy demand as the heating comes on, they cook dinner and turn on a range of applicances.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 Climate\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/intermittency.gif?raw=true\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These mismatched curves create periods where we have surplus power supply (green) and periods where we have a deficit (red). Where demand exceeds supply, as is the case in the evening, the national grid will cover the deficit by spinning up a gas-powered power station (a process called *peak-matching*), doing so creates unneccessary emissions that could have been avoided if we had shifted surplus power created during the day to the evening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 Climate\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/intermittency_xkcd.png?raw=true\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is an illusration of such an idealised scenario. Now we've added a storage medium, this could be a chemical storage i.e. a battery, or it could be thermal storage i.e. thermal inertia in building materials. We've flattened the blue demand curve, spreading it throughout the day rather than allowing the peaks in the morning and evenings. Now we store our surplus energy during the day in orange, and discharge it in the mornings and evening in blue. In this scenario we avoid the unnecessary spinning up of the gas-powered power plant and reduce overall emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 Climate\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/opt_intermittency_xkcd.png?raw=true\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Equipment in most buildings are regulated by Rule-based Controllers (RBCs) that take system temperature as input, use a temperature setpoint as an objective, and actuate equipment to minimise the error between objective and state. Though usefully simple, they don't maximise energy efficiency, nor do they interact with the grid to draw power when carbon intensity is low to minimise emissions. We'd like to design controllers that learn patterns in occupant behaviour, weather, and grid carbon intensity and take control actions that move us toward this idealised scenario that minimises emissions. For that, in my work, I employ Reinforcement Learning (RL), a method for obtaining optimal control polices in complex systems autonomously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.2 Commercial\n",
    "\n",
    "<img class=\"\" src=\"images/coldstore2.png\" align= 'left' style=\"width:41%\">\n",
    "\n",
    "\n",
    "<img class=\"\" src=\"images/slab_temps1.png\" style=\"width:60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So why is this relevant to Emerson? Where the storage medium in the residential example could be electrical, like a battery, it doesn't need to be, it could be thermal, like a fridge or cold storage facility. Here rather than charging a battery with excess renewable supply, we can charge the contents of the fridge with cooled air, and allow the heat to dissipate ove time in the same way we dissipate energy from the battery. As a more concrete example, if we could predict that we would have excess renewable supply throughout a morning because it was windy, and a deficit in the afternoon as the wind drops. We could use that renewable energy in the morning to overcool the space, then switch the coolers off in the afternoon, all whilst ensuring the temperature remains within a safe operating range throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.2 Commercial\n",
    "\n",
    "- Cooling equipment in 1000s of coldstorage facilities\n",
    "\n",
    "- Can we write control algorithms that improve:\n",
    "    - _Efficiency_\n",
    "    - _Flexibility_\n",
    "\n",
    "- **Grid-interactive Efficient Buildings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So we know Emerson have equipment in 1000s of these facilities globally. The questions then become, can we write control algorithms that improve the efficiency and the flexibility of these buildings. This is a concept often referred to as creating Grid-interactive efficient buildings. Given the scale of Emerson's global coldchain operations, doing so offers significant scope to reduce emissions, save money on energy bills for a client, and acrue revenue by offering the grid flexibility.\n",
    "\n",
    "Any questions at this stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Existing Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.1 Reinforcement Learning\n",
    "<img class=\"\" src=\"images/rl_basic.png\" style=\"width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Several authors have looked at this problem in recent years with most focussing on Reinforcement Learning as a potential solution. Reinforcement Learning is a general framework for solving sequential decision making tasks with no prior knowledge of the system. We have RL agents (here illustrated by the brain) that can take actions in some environment (represented by the globe), at each timestep receiving feedback on those actions in the form of a reward, which represents how well it is doing at task it is supposed to be performing, and an updated state. The goal of the RL agent is to maximise the sum of cumulative rewards over a trajectory of actions. This is useful because in complex systems it can be hard to specify a sequence of actions we would like an agent to take. Instead it is much easier to specify a goal we are trying to achieve or how we would like the world to look upon completing the task. We can do this by specifying the reward function, which is the only bit of human intervention in this system, then we allow the agent to work out the best control policy for maximising its reward.\n",
    "\n",
    "Although this appears a simple paradigm, it elicits extrememly powerful behaviour from computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/TmPfTpjtdgg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1059a4550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('TmPfTpjtdgg', height=400, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Some of the best examples of this power come from the gaming domain. Here‚Äôs an RL agent, created by DeepMind in 2013, that learns to play the Atari game breakout with no prior knowledge or human input. The agent is fed minimal data; it‚Äôs fed the raw pixels of the screen as a representation of the state; it‚Äôs knows it can take three actions: move left, move right, or stay still, and it‚Äôs fed the score at the top of the screen as a reward signal.\n",
    "\n",
    "You can see initially it struggles to hit the ball, but slowly it learns first to connect with the ball. Then by episode 600 it exhibits some pretty intelligent behaviour; it discovers that the optimally strategy is to burrow holes in the channels of the bricks and ping the ball behind the bricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-03-15-UselessGroupXchange/images/move37.png?raw=true\" style=\"width:40%\">\n",
    "\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-03-15-UselessGroupXchange/images/leesedol.png?raw=true\" style=\"width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Another seminal example is DeepMind's AlphaGo agent that beat the world-champion Go player Lee Sedol 4-1 back in 2016. Go is an extremely difficult game to master, historically seen as one of the hardest challenges for artificial intelligence. There are 10^170 ways to configure a Go board, meaning if you want to create a table of all the different states and search through them to find the best move you would need a computer larger than observable universe. The move highlighted in this image, move 37, became infamous in the Go world because it seemed counterintuitive to humans when played, but later turned out the be the game-winning move. AlphaGo managed to uncover new knowledge about a game that has been played by humans for thousands of years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.2 Our Game: Heating/Cooling Control\n",
    "\n",
    "### State-space\n",
    "- Indoor temperature/humidity\n",
    "- Outdoor temperature/humidity\n",
    "- Occupancy patterns (doors, lights, time, date)\n",
    "- Grid carbon intensity\n",
    "- Energy price\n",
    "- etc.\n",
    "\n",
    "### Action-space\n",
    "- Chiller/heater power\n",
    "\n",
    "### Reward\n",
    "- Minimise energy-use or emissions: $\\min{Energy_{t:T}}$ or $\\min{Emissions_{t:T}}$\n",
    "- subject to: $T_{low} \\leq T_t \\leq T_{high}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Instead of playing Go or the Atari Suite we want to formulate a game around building control. Our board or state-space becomes a feature space of indoor/outdoor temperature/humidity, occupany patterns, grid carbon intensity, energy price etc. The actions we can take relate, in the simplest case, to chiller and heater actuation i.e. we can turning the coolers up and down in response the state we observe. And our reward is based on minimising energy or emissions our some trajectory small t to big T, whilst maintaining temperature in a safe operating range. Past attempts to solve this game have focussed primarily on model-free reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.3 Model-free Reinforcement Learning\n",
    "\n",
    "Rollouts             |  Network\n",
    ":-------------------------:|:-------------------------:\n",
    "|\n",
    "![](https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/model_free.gif?raw=true)  |  ![](https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/model_free_network1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In model free RL, our agent learns policy that is a mapping from state to action, represented by the network diagram on the right. On the left, we visualise how an RL agent collects experience in an environment to learn this mapping. The agent will start in some state $s_t$ where it takes some action $a_t$ transitioning it to some next state $s_{t+1}$, from which it takes another action $a_{t+1}$. This process continues until some finite time horizon $H$ where it will receive some reward, a qunatification of how good the previous string of actions were. This reward is backpropogated, and the network is updated to reflect how good or bad each of those actions were at achieving our goal.\n",
    "\n",
    "As may be clear, to obtain an optimal solution, the agent would have to visit all possible states and take all possible actions. To do this safely, these agents are usually obtained in building simulations using millions of timesteps of data, but such simulations can be difficult, expensive or impossible to obtain in some buildings, so this approach limits the applicability of model-free RL to the subset of global buildings for which we have can obtain such data in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.4 Model-based Reinforcement Learning\n",
    "Rollouts             |  Network\n",
    ":-------------------------:|:-------------------------:\n",
    "|\n",
    "![](https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/model_based.gif?raw=true)  |  ![](https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/model_based_network.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "An alternative is model-based RL where the agent instead learns a mapping from state-action pair to next state, in other words, it attempts to learn the system dynamics. Using this model, the agent can test a range of candidate action sequences, estimating the expected reward of each at time horizon $H$. The agent then takes the first action from the action sequence producing highest expected reward in the real world. \n",
    "\n",
    "This approach is considerably more data-efficient than model-free RL because the agent can imagine the consequences of as many actions as it likes offline, limiting the amount of iteractions required in the real environment to obtain a good policy. Because of this, authors have shown that model-based RL agents do not need to be pre-trained in simulation, but can instead be deployed directly in the environment where it can learn a policy after 12 hours of interaction. However, the primiary drawback is that the model it learns is inevitably wrong to some degree, or *biased*, and this model inaccuracy can lead to poorer performance than model-based agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|                | **Pros** üíö             | **Cons** üíî                         |\n",
    "|----------------|------------------|------------------------------|\n",
    "|**Model-free**  | Performance | Data inefficiency    |\n",
    "|**Model-based** | Data efficiency       | Performance; Model bias      |\n",
    "\n",
    "|                | **SOA Energy Efficiency**             | **SOA Data Efficiency**         | **Authors** |\n",
    "|----------------|------------------|------------------------------|----------------------|\n",
    "|**Model-free**  | **10-30%**  |  1-2 years *simulated* data   | [2,3] |\n",
    "|**Model-based** | 5-9%       | **3-12 hours** *live* data      | [4, 5] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here I summarise the pros and cons of each approach. In general, model-free approaches perform better but are data inefficient - requiring pre-training in a simulator. On the other hand, model-based techniques are far more data efficient, but model-bias can cause poorer performance. We see this reflected in the building control literature, where model-free approaches have shown energy efficiency improvements of up to 30\\%, whereas model-based techniques have only shown up to 9\\% energy efficiency improvements.\n",
    "\n",
    "Ideally, we'd like to find new solutions that elicit the performance of model-free approaches with the data efficiency and online commissioning of model-based approaches. The approach we propose for doing so is *Probabilistic* Model-Based Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. *Probabilisitic* Model-Based Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we follow the same schema as traditional model-based RL i.e. we learn a mapping from state-action pair to next state, but now instead of deterministically predicting the state evolution, we learn a probability distribution over next state paramterised by mean $\\mu$ and variance $\\sigma$. We employ an ensemble of 5 probabilistic networks to account for uncertainty in the modelling approach too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/prob_rl.png?raw=true\" style=\"width:60%\">\n",
    "\n",
    "after [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The method for agent planning changes too, now instead of making point predictions of the outcome of action sequences, we output a distribution over all possible future rewards. In the example below, we evaluate one action sequence, passing it through three models in our ensemble, outputting state distribution predictions, sampling from that distribution and iterating until time horizon $H$. At this stage we get a distribution over rewards which we can use to get the expected value of the action sequence. Comparing this with the expected reward of many other action sequences we can find the optimal action sequence, and take the first action in that sequence in the real environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/prob_rl.gif?raw=true\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We ran a series of experiments to test the effectiveness of our proposed approach. We test our algorithm in three varied building simulations, a mixed-use facility in Greece, an apartment block in Spain, and a seminar centre in Denmark. We compare our algorithm with an existing RBC, the state-of-the-art model-free approach and the state-of-the-art model-based approach. The agents have no prior knowledge of the system *a priori*, other than the type of actions they can take, and are each given 12 hours to explore the state-space before controlling them. The goal of each agent is to minimise emissions whilst maintaining internal temperature in the range 19-24 degrees centigrade for one year of operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.1 Building Simulations [7]\n",
    "\n",
    "<span style=\"color:white\">some *blue* text</span>             |  <span style=\"color:white\">some *blue* text</span> | <span style=\"color:white\">some *blue* text</span>\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|\n",
    "|||\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/mixed-use.PNG?raw=true\" style=\"width:100%\">|<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/apartment.png?raw=true\" style=\"width:80%\">|<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/seminar_centre.PNG?raw=true\" style=\"width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We find that in two of the three cases our PMBRL method (green) minimises emissions over all other controllers. In the mixed-use facility it reduces emissions by 30\\% over the RBC and in the apartment block it reduces emissions by 10\\%. In the seminar centre it creates the same volume of emissions as the existing controller.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.2 Emissions\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/cum_emissions_1.png?raw=true\" style=\"width:100%\">\n",
    "\n",
    "***Figure 1:*** *Cumulative emissions produced by each controller across the (a) Mixed Use, (b) Apartment, and (c) Seminar Centre environments. Green: PMBRL; Orange: PPO; Blue: MPC-DNN; Red: RBC.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our approach is also the best at maintaining temperature within the desired bounds, here represented by the light green band. Our approach never breaches the temperature bounds in the apartment or seminar centre, and does so 0.27/% of the year in the mixed use facility. In contrast the existing SOA approaches breach the same bounds 18.36/% (model-based) and 53.33/% (model-free) of the time on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.3 Thermal Comfort\n",
    "<img class=\"\" src=\"https://github.com/enjeeneer/talks/blob/main/2021-11-17-CambridgeZero/images/temperature_control.png?raw=true\" style=\"width:100%\">\n",
    "\n",
    "***Figure 2:*** *Mean daily building temperature produced by each controller across the (a) Mixed Use, (b) Apartment, and (c) Seminar Centre environments. Green: PMBRL; Orange: PPO; Blue: MPC-DNN; Red: RBC; Purple Dashed: Outdoor temperature (the primary system disturbance). The green shaded area illustrates the target temperature range [19, 24].*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Conclusions\n",
    "\n",
    "1. Reducing energy use in heating and cooling systems, and better matching energy demand with carbon-free supply are important climate change mitigation tools.\n",
    "2. In theory, RL allows us to obtain emission-reducing control policies for *any* building with *no* prior knowledge.\n",
    "3. Existing RL approaches require training in hard-to-obtain simulators using millions of timesteps of data.\n",
    "\n",
    "\n",
    "4. **Our approach, PMBRL, is easy to deploy, requiring no prior knowledge, and can obtain emission-reducing control policies after only a few hours of interaction in the real environment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks!\n",
    "\n",
    "<img class=\"\" src=\"images/qr.svg\" style=\"width:30%\" align='left'>\n",
    "\n",
    "\n",
    "\n",
    "### Twitter: [@enjeeneer](https://twitter.com/enjeeneer)\n",
    "\n",
    "### Website: [https://enjeeneer.io/](https://enjeeneer.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. References\n",
    "\n",
    "[1] Cullen, Jonathan and Julian Allwood (2010). ‚ÄúThe efficient use of energy: Tracing the global flow of energy from fuel to service‚Äù. In: Energy Policy 38.1, pp. 75‚Äì81.\n",
    "\n",
    "[2] Zhang, Z., Chong, A., Pan, Y., Zhang, C., Lam, K.P.: Whole building energy model for hvac optimalcontrol: A practical framework based on deep reinforcement learning. Energy and Buildings199,472‚Äì490 (2019)\n",
    "\n",
    "[3] Wei, T., Wang, Y., Zhu, Q.: Deep reinforcement learning for building hvac control. In: Proceedingsof  the  54th  Annual  Design  Automation  Conference  2017.  DAC  ‚Äô17,  Association  for  ComputingMachinery, New York, NY, USA (2017). \n",
    "\n",
    "[4] Lazic, N., Lu, T., Boutilier, C., Ryu, M., Wong, E.J., Roy, B., Imwalle, G.: Data center cooling usingmodel-predictive control. In: Proceedings of the Thirty-second Conference on Neural InformationProcessing Systems (NeurIPS-18). pp. 3818‚Äì3827. Montreal, QC (2018)\n",
    "\n",
    "[5] Jain, A., Smarra, F., Reticcioli, E., D‚ÄôInnocenzo, A., Morari, M.: Neuropt: Neural network basedoptimization for building energy management and climate control. In: Bayen, A.M., Jadbabaie, A.,Pappas, G., Parrilo, P.A., Recht, B., Tomlin, C., Zeilinger, M. (eds.) Proceedings of the 2nd Confer-ence on Learning for Dynamics and Control. Proceedings of Machine Learning Research, vol. 120,pp. 445‚Äì454\n",
    "\n",
    "[6] Chua,  K.,  Calandra,  R.,  McAllister,  R.,  Levine,  S.:  Deep  reinforcement  learning  in  a  handful  oftrials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114 (2018)\n",
    "\n",
    "[7] Scharnhorst, P., Schubnel, B., Fern ÃÅandez Bandera, C., Salom, J., Taddeo, P., Boegli, M., Gorecki, T.,Stauffer, Y., Peppas, A., Politi, C.: Energym: A building model library for controller benchmarking.Applied Sciences11(8),  3518 (2021)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
