---
title: "On Zero-Shot Reinforcement Learning"
author: "Scott Jeen"
subtitle: PhD Viva
institute: University of Cambridge
format: rick-revealjs
---

## Outline

:::{.fragment}
1. The Case for Zero-Shot RL
:::

:::{.fragment}
2. **Chapter 3:** From Low Quality Data
:::

:::{.fragment}
3. **Chapter 4:** Under Changed Dynamics
:::

## Outline

1. The Case for Zero-Shot RL

2. **Chapter 3:** From Low Quality Data

3. <span style="color: red;">~~**Chapter 4:** Under Changed Dynamics~~</span>

:::{.fragment}
3. **Chapter 4:** Under <span style="color: green;">_Partial Observability_</span>
:::

:::{.fragment}
4. **Chapter 5:** With No Prior Data
:::

:::{.fragment}
5. Outlook
:::

## The Case for RL

:::{.fragment}
- Society faces problems, many of which can be cast as sequential decision-making problems:
:::
:::{.fragment}
    - Engineering: energy generation control (fusion, fission, wind)
:::
:::{.fragment}
    - Education: teacher-student interaction
:::
:::{.fragment}
    - Mathematics: theorem-proving
:::
:::{.fragment}
    - Policy: climate negotiations
:::
:::{.fragment}
    - Science: forming hypotheses -> making predictions -> testing them
:::

:::{.fragment}
- In such cases:
:::
:::{.fragment}
    - System dynamics are rarely known (so standard control theoretic approaches can't be applied)
:::
:::{.fragment}
    - Data demonstrating the optimal policy is rarely available (so we can't imitate the optimal policy with supervised learning)
:::
:::{.fragment}
    - It is much easier to evaluate a solution that generate one (i.e. there exists a generator-verifier gap)
:::

:::{.fragment}
- RL provides a generic framework for modelling sequential-decision making problems with these characteristics [1]
:::

---

## RL + _Perfect_ Simulators + Compute = Superhuman

<div style="text-align: center;">
![](figures/silver-rl.png){width=100%}
</div>

::: aside
David Silver, RL Conference 2024
:::

---

## RL + <span style="color: red;">~~_Perfect_ Simulators~~</span> + Compute = Superhuman

<div style="text-align: center;">
![](figures/silver-rl.png){width=100%}
</div>

::: aside
David Silver, RL Conference 2024
:::

---

## RL + _Learned Simulators_ + Compute = Meh?

:::{.fragment}
- In the absence of a perfect simulator, we can _learn_ one from data
:::

:::{.fragment}
- In practice, this means collecting data from our problem's environment, and building a model that simulates its dynamics
:::

:::{.fragment}
- But these models will only ever _approximate_ the environment's dynamics
:::

:::{.fragment}
    - Finite data
:::

:::{.fragment}
    - Function approximation
:::

:::{.fragment}
- So, a discrepancy between the dynamics modelled by learned simulators and the dynamics of the problem environment is **inevitable**
:::

---

## The Case for Zero-Shot RL
- _Zero-shot RL_ methods aim to handle this discrepancy quickly

:::{.fragment}
- Impressive progress has been made if the gap between the learned simulator and the real-world is small
:::

:::{.fragment}
- I contend that to solve real-world problems these methods need to deal with a larger gap and satisfy:
<div style="text-align: center;">
![](figures/constraints.png){width=80%}
</div>
:::

---

<div style="padding-top: 200px;">
<div style="text-align: center;">
![](figures/thesis.png){width=85%}
</div>
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Motivation
:::{.fragment}
- The standard zero-shot RL methods, forward-backward representations (FB) [2] and universal successor features (USF) [3], provide principled mechanisms for performing zero-shot task generalisation
:::

:::{.fragment}
- However, they assume access to idealised (large & diverse) pre-training datasets that we can't expect for real problems
:::

:::{.fragment}
- **Can we pre-train zero-shot RL methods on realistic (small & narrow) datasets?**
:::

::: aside
**Scott Jeen**, Tom Bewley, and Jonathan M. Cullen. _Zero-shot Reinforcement Learning from Low Quality Data_. In Advances in Neural Information Processing Systems 38, 2024
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Failure mode: out-of-distribution value overestimation [4]

<div style="text-align: center;">
![](figures/chapter3-overestimates.png){width=85%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Failure mode: out-of-distribution value overestimation [4]

<div style="text-align: center;">
![](figures/chapter3-overestimates.png){width=85%}
</div>

<div style="text-align: center;">
![](figures/chapter3-fb-loss.png){width=100%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Failure mode: out-of-distribution value overestimation [4]

<div style="text-align: center;">
![](figures/chapter3-overestimates.png){width=85%}
</div>

<div style="text-align: center;">
![](figures/chapter3-fb-loss-2.png){width=100%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-0.png){width=90%}

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-1.png){width=90%}

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-2.png){width=90%}

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-3.png){width=100%}

![](figures/chapter3-vcfb-equation.png){width=77%}
![](figures/chapter3-mcfb-equation.png){width=100%}

::: aside
C.f. [5]
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### ExORL Results

::: {.columns}
:::: {.column width="50%"}

#### Baselines
- **Zero-shot RL:** FB [2], SF-LAP [6]
- **Goal-conditioned RL:** GC-IQL [7]
- **Offline RL:** CQL [5]

::::

:::: {.column width="50%"}
#### Datasets

![](figures/chapter3-dataset-heatmap.png){width=100%}
::::
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### ExORL Results

<div style="padding-top: 50px;">
![](figures/chapter3-exorl.png){width=100%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### D4RL Results

<div style="padding-top: 50px;">
<div style="text-align: center;">
![](figures/chapter3-d4rl.png){width=52%}
</div>
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Performance on Idealised Datasets is Unaffected

::: {.columns}
::: {.column width="50%"}
![](figures/chapter3-dataset-size.png)
:::
::: {.column width="50%"}
![](figures/chapter3-full-datasets.png)
:::
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Conclusions

::: {.fragment}
- Like standard offline RL methods, zero-shot RL methods suffer from the _distribution shift_
:::

::: {.fragment}
- As a resolution, we introduce _Conservative_ zero-shot RL methods
:::

::: {.fragment}
- _Conservative_ zero-shot RL methods considerably outperform standard zero-shot RL methods on low-quality datasets
:::

::: {.fragment}
- _Conservative_ zero-shot RL methods do not compromise performance on idealised datasets
:::

---

## Chapter 4: Zero-shot RL under Partial Observability

---

## Paper 3: With No Prior Data (for building control)
<div style="padding-top: 100px;">
<div style="text-align: center;">
![](figures/pearl.png){width=80%}
</div>
</div>

::: aside
Scott Jeen, Alessandro Abate & Jonathan M. Cullen. _Low Emission Building Control with Zero-Shot Reinforcement Learning_. AAAI 2023
:::

---

## Paper 3: With No Prior Data (for building control)
<div style="padding-top: 20px;">
<div style="text-align: center;">
![](figures/emissions.jpg){width=80%}
</div>
</div>

::: aside
Scott Jeen, Alessandro Abate & Jonathan M. Cullen. _Low Emission Building Control with Zero-Shot Reinforcement Learning_. AAAI 2023
:::

---

## Outlook

---

# Thanks!

---

## References

::: references

[1] Sutton, R. and Barto, A. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.

[2] Touati, A. and Ollivier, Y. (2021). Learning one representation to optimize all rewards. Advances in Neural Information Processing Systems, 34:13â€“23

[3] Borsa, D., Barreto, A., Quan, J., Mankowitz, D., Munos, R., Van Hasselt, H., Silver, D., and Schaul, T. (2018). Universal successor features approximators. arXiv preprint arXiv:1812.07626.

[4] Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019). Stabilizing off-policy q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, volume 32

[5] Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative q-learning for offline reinforcement learning. arXiv preprint arXiv:2006.04779.

[6] Touati, A., Rapin, J., and Ollivier, Y. (2023). Does zero-shot reinforce- ment learning exist? In The Eleventh International Conference on Learning Representations.

[7] Park, S., Ghosh, D., Eysenbach, B., and Levine, S. (2023). Hiql: Offline goal- conditioned rl with latent states as actions. Advances in Neural Information Processing Systems, 37.

:::

---
