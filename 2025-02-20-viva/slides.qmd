---
title: "On Zero-Shot Reinforcement Learning"
author: "Scott Jeen"
subtitle: PhD Viva
institute: University of Cambridge
format: rick-revealjs
footer: "[https://enjeeneer.io](https://enjeeneer.io)"
---

## Outline

:::{.fragment}
1. The Case for Zero-Shot RL
:::

:::{.fragment}
2. **Chapter 3:** From Low Quality Data
:::

:::{.fragment}
3. **Chapter 4:** Under Changed Dynamics
:::

## Outline

1. The Case for Zero-Shot RL

2. **Chapter 3:** From Low Quality Data

3. <span style="color: red;">~~**Chapter 4:** Under Changed Dynamics~~</span>

:::{.fragment}
3. **Chapter 4:** Under <span style="color: green;">_Partial Observability_</span>
:::

:::{.fragment}
4. **Chapter 5:** With No Prior Data
:::

:::{.fragment}
5. Outlook
:::

## The Case for RL

:::{.fragment}
- Society faces problems, many of which can be cast as sequential decision-making problems:
:::
:::{.fragment}
    - Engineering: energy generation control (fusion, fission, wind)
:::
:::{.fragment}
    - Education: teacher-student interaction
:::
:::{.fragment}
    - Mathematics: theorem-proving
:::
:::{.fragment}
    - Policy: climate negotiations
:::
:::{.fragment}
    - Science: forming hypotheses -> making predictions -> testing them
:::

:::{.fragment}
- In such cases:
:::
:::{.fragment}
    - System dynamics are rarely known (so standard control theoretic approaches can't be applied)
:::
:::{.fragment}
    - Data demonstrating the optimal policy is rarely available (so we can't imitate the optimal policy with supervised learning)
:::
:::{.fragment}
    - It is much easier to evaluate a solution that generate one (i.e. there exists a generator-verifier gap)
:::

:::{.fragment}
- RL provides a generic framework for modelling sequential-decision making problems with these characteristics [1]
:::

---

## RL + _Perfect_ Simulators + Compute = Superhuman

<div style="text-align: center;">
![](figures/silver-rl.png){width=100%}
</div>

::: aside
David Silver, RL Conference 2024
:::

---

## RL + <span style="color: red;">~~_Perfect_ Simulators~~</span> + Compute = Superhuman

<div style="text-align: center;">
![](figures/silver-rl.png){width=100%}
</div>

::: aside
David Silver, RL Conference 2024
:::

---

## RL + _Learned Simulators_ + Compute = Meh?

:::{.fragment}
- In the absence of a perfect simulator, we can _learn_ one from data
:::

:::{.fragment}
- In practice, this means collecting data from our problem's environment, and building a model that simulates its dynamics
:::

:::{.fragment}
- But these models will only ever _approximate_ the environment's true dynamics
:::

:::{.fragment}
    - Finite data
:::

:::{.fragment}
    - Function approximation
:::

:::{.fragment}
- So, a discrepancy between the dynamics modelled by learned simulators and the dynamics of the problem environment is **inevitable**
:::

---

## The Case for Zero-Shot RL
- _Zero-shot RL_ methods aim to handle this discrepancy quickly

:::{.fragment}
- Impressive progress has been made if the gap between the learned simulator and the real-world is small
:::

:::{.fragment}
- I contend that to solve real-world problems these methods need to deal with a larger gap and satisfy:
<div style="text-align: center;">
![](figures/constraints.png){width=80%}
</div>
:::

---

<div style="padding-top: 200px;">
<div style="text-align: center;">
![](figures/thesis.png){width=85%}
</div>
</div>

---

# Chapter 3: <br> Zero-shot RL _from Low Quality Data_

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Motivation
:::{.fragment}
- The standard zero-shot RL methods, forward-backward representations (FB) [2] and universal successor features (USF) [3], provide principled mechanisms for performing zero-shot task generalisation
:::

:::{.fragment}
- However, they assume access to idealised (large & diverse) pre-training datasets that we can't expect for real problems
:::

:::{.fragment}
- **Can we pre-train zero-shot RL methods on realistic (small & narrow) datasets?**
:::

::: aside
**Scott Jeen**, Tom Bewley, and Jonathan M. Cullen. _Zero-shot Reinforcement Learning from Low Quality Data_. In Advances in Neural Information Processing Systems 38, 2024
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Failure mode: out-of-distribution value overestimation [4]

<div style="text-align: center;">
![](figures/chapter3-overestimates.png){width=85%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Failure mode: out-of-distribution value overestimation [4]

<div style="text-align: center;">
![](figures/chapter3-overestimates.png){width=85%}
</div>

<div style="text-align: center;">
![](figures/chapter3-fb-loss.png){width=100%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Failure mode: out-of-distribution value overestimation [4]

<div style="text-align: center;">
![](figures/chapter3-overestimates.png){width=85%}
</div>

<div style="text-align: center;">
![](figures/chapter3-fb-loss-2.png){width=100%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-0.png){width=90%}

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-1.png){width=90%}

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-2.png){width=90%}

---

## Chapter 3: Zero-shot RL from Low Quality Data

### _Conservative_ Zero-shot RL
![](figures/chapter3-vcfb-intuition-3.png){width=100%}

![](figures/chapter3-vcfb-equation.png){width=77%}
![](figures/chapter3-mcfb-equation.png){width=100%}

::: aside
C.f. [5]
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### ExORL Experiments

::: {.columns}
:::: {.column width="50%"}

#### Baselines
- **Zero-shot RL:** FB [2], SF-LAP [6]
- **Goal-conditioned RL:** GC-IQL [7]
- **Offline RL:** CQL [5]

::::

:::: {.column width="50%"}
#### Datasets

![](figures/chapter3-dataset-heatmap.png){width=100%}
::::
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### ExORL Results

<div style="padding-top: 50px;">
![](figures/chapter3-exorl.png){width=85%}
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### D4RL Results

<div style="padding-top: 50px;">
<div style="text-align: center;">
![](figures/chapter3-d4rl.png){width=45%}
</div>
</div>

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Performance on Idealised Datasets is Unaffected

::: {.columns}
::: {.column width="50%"}
![](figures/chapter3-dataset-size.png)
:::
::: {.column width="50%"}
![](figures/chapter3-full-datasets.png)
:::
:::

---

## Chapter 3: Zero-shot RL from Low Quality Data

### Conclusions

::: {.fragment}
- Like standard offline RL methods, zero-shot RL methods suffer from the _distribution shift_
:::

::: {.fragment}
- As a resolution, we introduce _conservative_ zero-shot RL methods
:::

::: {.fragment}
- _Conservative_ zero-shot RL methods considerably outperform standard zero-shot RL methods on low-quality datasets
:::

::: {.fragment}
- _Conservative_ zero-shot RL methods do not compromise performance on idealised datasets
:::

---

# Chapter 4: <br> Zero-shot RL _under Partial Observability_

---

## Chapter 4: Zero-shot RL under Partial Observability
### Motivation

:::{.fragment}
- The standard zero-shot RL methods assume the assume the environment is _fully observed_.
:::

:::{.fragment}
- In many real-world problems the state is _partially observed_. Indeed, many other important problems in RL are naturally formulated as partially observed MDPs e.g. generalisation, temporal credit assignment etc.
:::

:::{.fragment}
- **Can we pre-train zero-shot RL methods to perform well under partial observability?**
:::

::: aside
**Scott Jeen**, Tom Bewley, and Jonathan M. Cullen. _Zero-shot Reinforcement Learning Under Partial Observability_.
:::

---

## Chapter 4: Zero-shot RL under Partial Observability
### Failure mode: state and task mis-identification

<div style="text-align: center;">
![](figures/chapter4-fb-pomdp-failure-mode.png){width=70%}
</div>

<div style="text-align: center;">
![](figures/chapter4-task-misid-equation.png){width=50%}
</div>
<div style="text-align: center;">
![](figures/chapter4-state-misid-equation.png){width=40%}
</div>
<div style="text-align: center;">
![](figures/chapter4-both-misid-equation.png){width=43%}
</div>

---

## Chapter 4: Zero-shot RL under Partial Observability
### Memory-based Zero-shot RL methods

<div style="padding-top: 70px;">
<div style="text-align: center;">
![](figures/chapter4-rfb-architecture.png){width=90%}
</div>
</div>
---

## Chapter 4: Zero-shot RL under Partial Observability
### ExORL Experiments

::: {.columns}
:::: {.column width="50%"}

#### Baselines
- **Zero-shot RL:** FB [2], HILP [6], FB-stack
- **Oracle:** FB-MDP
::::

:::: {.column width="50%"}
#### Tests
- **Standard POMDP:** Noisey ExORL
- **Generalisation:** ExORL with dynamics interp./extrap.
::::
:::

---

## Chapter 4: Zero-shot RL under Partial Observability
### Standard POMDPs

<div style="padding-top: 30px;">
<div style="text-align: center;">
![](figures/chapter4-aggregate-noisey-environment.png){width=70%}
</div>
</div>

---

## Chapter 4: Zero-shot RL under Partial Observability
### Generalisation

<div style="padding-top: 30px;">
<div style="text-align: center;">
![](figures/chapter4-aggregate-generalisation.png){width=70%}
</div>
</div>

---

## Chapter 4: Zero-shot RL under Partial Observability
### Summary of proposed updates

::: {.fragment}
- _Both_ the forward and backward models are recurrent to handle state and task mis-identification
:::

::: {.fragment}
- Focus on dynamics generalisation (with Markov states) as a special case of partial observability
:::

::: {.fragment}
- Remove environment generalisation results
:::

---

## Chapter 4: Zero-shot RL under Partial Observability
### Conclusions

::: {.fragment}
- Real-world problems are often partially observed, yet zero-shot RL methods cannot handle partial observability out-of-the-box
:::

::: {.fragment}
- As a resolution, we introduce _memory-based_ zero-shot RL methods
:::

::: {.fragment}
- _Memory based_ zero-shot RL methods considerably outperform vanilla zero-shot RL methods on standard POMDPs, and show improved performance in the generalisation setting
:::

---

# Chapter 5: Zero-shot RL with No Prior Data

---

## Chapter 5: Zero-shot RL with No Prior Data
### Motivation

:::{.fragment}
- For many real-world problems we have access to neither a simulator nor historical data logs.
:::

:::{.fragment}
- Emission-efficient building control is one such problem. But all prior works have assumed simulators or historical data logs are available.
:::

:::{.fragment}
- **Can we learn to safely/efficiently control energy-intensive equipment in buildings *online* without any prior data?**
:::

::: aside
**Scott Jeen**, Alessandro Abate, and Jonathan M. Cullen. _Low Emission Building Control with Zero-Shot Reinforcement Learning_. AAAI 2023
:::

---

## Chapter 5: Zero-shot RL with No Prior Data
### Low Emission Building Control

<img class="" src="https://github.com/enjeeneer/talks/blob/main/2022-06-23-OXCAV/images/intermittency.gif?raw=true" style="width:55%">

---

## Chapter 5: Zero-shot RL with No Prior Data
### Low Emission Building Control

<div style="padding-top: 100px;">
<div style="display: flex; justify-content: center; gap: 20px;">
  <img src="figures/chapter5-intermittency_xkcd.png" width="48%">
  <img src="figures/chapter5-opt_intermittency_xkcd.png" width="48%">
</div>
</div>

---

## Chapter 5: Zero-shot RL with No Prior Data
### Failure Mode of Existing Methods: Data (In)efficiency

&nbsp;
&nbsp;
&nbsp;
&nbsp;

<div style="padding-top: 50px;">

|  **Authors**   | **Building** | **Algo** |   **Efficiency**             | **Data** |
|----------------|------------------|------------|-----------------------|----------------------|
|Wei et al. (2017) [9]  | 5-zone Building | DQN | ~35%  |  ~8 years   |
|Zhang et al. (2019) [10] | Office | A3C | ~17%       | ~30 years       |
| Valladares et al. (2019) [11]| Classroom  | DQN | 5%| ~10 years|

: {tbl-colwidths="[35, 25, 10, 10, 20]" .striped .hover}
</div>

---

## Chapter 5: Zero-shot RL with No Prior Data
### PEARL: Probabilistic Emission Abating Reinforcement Learning

<div style="padding-top: 50px;">
<div style="text-align: center;">
![](figures/chapter5-pearl.png){width=90%}
</div>
</div>

---

## Chapter 5: Zero-shot RL with No Prior Data{.scrollable}
### Energym Experiments

:::{.fragment}
#### Environments [15]


|                            | **Mixed-Use**               | **Offices**     | **Seminar Centre** |
|----------------------------|-----------------------------|-----------------|--------------------|
| **Location**               | Greece              | Greece  | Denmark   |
| **Floor Area (m$^2$)**      | 566                      | 643          | 1278            |
| **Action-space dim**      | $\mathbb{R}^{12}$             | $\mathbb{R}^{14}$ | $\mathbb{R}^{18}$    |
| **State-space dim**        | $\mathbb{R}^{37}$             | $\mathbb{R}^{56}$ | $\mathbb{R}^{59}$    |
| **Equipment** | Thermostats & AHU Flowrates | Thermostats     | Thermostats        |
: {tbl-colwidths="[50, 25, 20, 20]" .striped .hover}
:::

:::{.fragment}
#### Baselines
- SAC [12]
- PPO [13]
- MPC-DNN [14]
- RBC (a generic controller)
:::

---

## Chapter 5: Zero-shot RL with No Prior Data
### Energym Results

<div style="padding-top: 10px;">
<div style="text-align: center;">
![](figures/chapter5-emissions-table.png){width=65%}
</div>
</div>

---

## Chapter 5: Zero-shot RL with No Prior Data
### System Identification

<div style="padding-top: 50px;">
<div style="text-align: center;">
![](figures/chapter5-systemid.png){width=50%}
</div>
</div>

---

## Chapter 5: Zero-shot RL with No Prior Data

### Conclusions

::: {.fragment}
- Standard deep RL methods are too data inefficient to be deployed for zero-shot building control
:::

::: {.fragment}
- We introduce PEARL, a method that can be deployed for zero-shot for building control
:::

::: {.fragment}
- PEARL outperforms the RBC in 1/3 environments and standard deep RL method in all environments
:::

---

# Conclusions

---

## Conclusions

I set out to defend the following thesis:

::: {.fragment}
<div style="padding-top: 1px;">
<div style="text-align: center;">
![](figures/thesis.png){width=80%}
</div>
</div>
:::

::: {.fragment}
- In **Chapter 3**, I took steps toward addressing the _data quality_ constraint with conservative zero-shot RL methods
:::

::: {.fragment}
- In **Chapter 4**, I took steps toward addressing the _observability_ constraint with memory-based zero-shot RL methods
:::

::: {.fragment}
- In **Chapter 5**, I took steps to addressing the _data availability_ constraint (in the context of buidling control)  with PEARL
:::


---

# Looking forward to the discussion!

---

## References{.scrollable}

::: references

[1] Sutton, R. and Barto, A. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.

[2] Touati, A. and Ollivier, Y. (2021). Learning one representation to optimize all rewards. Advances in Neural Information Processing Systems, 34:13–23

[3] Borsa, D., Barreto, A., Quan, J., Mankowitz, D., Munos, R., Van Hasselt, H., Silver, D., and Schaul, T. (2018). Universal successor features approximators. arXiv preprint arXiv:1812.07626.

[4] Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019). Stabilizing off-policy q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, volume 32

[5] Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative q-learning for offline reinforcement learning. arXiv preprint arXiv:2006.04779.

[6] Touati, A., Rapin, J., and Ollivier, Y. (2023). Does zero-shot reinforce- ment learning exist? In The Eleventh International Conference on Learning Representations.

[7] Park, S., Ghosh, D., Eysenbach, B., and Levine, S. (2023). Hiql: Offline goal- conditioned rl with latent states as actions. Advances in Neural Information Processing Systems, 37.

[8] Park, S., Kreiman, T., and Levine, S. (2024). Foundation policies with hilbert representations. International Conference on Machine Learning.

[9] Wei, T., Wang, Y., and Zhu, Q. (2017). Deep reinforcement learning for building hvac control. In Proceedings of the 54th Annual Design Automation Conference 2017, DAC ’17, New York, NY, USA. Association for Computing
Machinery.

[10] Zhang, Z., Chong, A., Pan, Y., Zhang, C., and Lam, K. P. (2019b). Whole building energy model for hvac optimal control: A practical framework based on deep reinforcement learning. Energy and Buildings, 199:472–490.

[11] Valladares, W., Galindo, M., Gutiérrez, J., Wu, W.-C., Liao, K.-K., Liao, J.-C., Lu, K.-C., and Wang, C.-C. (2019). Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm. Building and Environment, 155:105 – 117.

[12] Haarnoja, T., Ha, S., Zhou, A., Tan, J., Tucker, G., and Levine, S. (2018a). Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103.

[13] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

[14] Nagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2018c). Neural network dynamics for model-based deep reinforcement learning with model- free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559–7566. IEEE.

[15] Scharnhorst, P., Schubnel, B., Fernández Bandera, C., Salom, J., Taddeo, P., Boegli, M., Gorecki, T., Stauffer, Y., Peppas, A., and Politi, C. (2021). Energym: A building model library for controller benchmarking. Applied Sciences, 11(8):3518.

:::

---
